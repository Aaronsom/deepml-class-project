\documentclass[article,accentcolor=tud8d,bibliography=totoc]{tudreport}
\usepackage[stable]{footmisc}
\usepackage{hyperref}

\usepackage{longtable}
\usepackage{multirow}
\usepackage{booktabs}

\hypersetup{%
  pdftitle={Multilingual Machine Translation with the Transformer Model},
  pdfauthor={Gregor Geigle, Hadsy Cardenas and Timur Levent Görgü},
  pdfsubject={Exposé for DeepML Class Project - Multilingual Transformer},
  pdfview=FitH,
  pdfstartview=FitV
}

\setcounter{seclinedepth}{1}

%%% Zum Tester der Marginalien %%%
  \newif\ifTUDmargin\TUDmarginfalse
  %%% Wird der Folgende Zeile einkommentiert,
  %%% werden Marginalien gesetzt.
  % \TUDmargintrue
  \ifTUDmargin\makeatletter
    \TUD@setmarginpar{2}
  \makeatother\fi
%%% ENDE: Zum Tester der Marginalien %%%

\newlength{\longtablewidth}
\setlength{\longtablewidth}{0.7\linewidth}
\addtolength{\longtablewidth}{-\marginparsep}
\addtolength{\longtablewidth}{-\marginparwidth}

\usepackage[
backend=biber, % biber ist das Standard-Backend für Biblatex. Für die Abwärtskompatibilität kann hier auch bibtex oder bibtex8 gewählt werden (siehe biblatex-Dokumentation)
style=numeric, %numeric, authortitle, alphabetic etc.
sorting=nty, % Sortierung: nty = name title year, nyt = name year title u.a.
sortcase=false,
url=false,
hyperref=auto,
]{biblatex}
\addbibresource{References.bib}
% \settitlepicture{tudreport-pic}
% \printpicturesize

\title{Multilingual Machine Translation with the Transformer Model}
\subtitle{Gregor Geigle, Hadsy Cardenas and Timur Levent Görgü}
%\setinstitutionlogo[width]{TUD_sublogo}
\begin{document}
\maketitle
\begin{abstract}
In this project, we plan to implement a Transformer model and train it for multilingual machine translation supporting two to five languages.
Multilingual models have shown good results compared to bilingual models and they are possibly an efficient solution to supporting many different languages. 
The recently proposed Transformer architecture seems to perform better than the previous RNN-based architectures for machine translation and other language tasks.
\end{abstract}  

\tableofcontents

\section{Multilingual Machine Translation}
Translating text from one language is an important task in our modern global world.
This is especially pronounced here in the European Union. 
There are 24 official languages\footnote{\url{https://europa.eu/european-union/about-eu/eu-languages_en}} and, for example, Parliament speeches need to be translated in each language.
Neural Machine Translation (NMT) has shown great promise performing the task automatically with better results than previous Machine Translation approaches \autocite{DBLP:journals/corr/WuSCLNMKCGMKSJL16}.\\
These neural systems are usually trained bilingually for one language pair.
Supporting many languages requires thus one system for each language pair.
Recently, however, models have been proposed for universal many-to-many systems which can translate directly from any language to another \autocite{DBLP:journals/corr/HaNW16,johnson2017google}.\\
These multilingual models reduce the number of required systems and they only need one set of parameters which can simplify the development. 
They also can improve the translation quality of low-resource languages and might even enable zero-shot translation, that is translation from languages not seen during training \autocite{DBLP:journals/corr/abs-1903-00089}.\\
Large scale studies suggest that these multilingual systems might be able to outperform bilingual systems \autocite{DBLP:journals/corr/abs-1806-06957,DBLP:journals/corr/abs-1903-00089}.

\section{The Transformer}
Recurrent Neural Networks (RNN) have been the go-to architecture for NMT as they work well for sequential data like sentences.
They are usually combined with Attention mechanisms to better model the dependencies between the sentence parts.
However, RNNs are hard to parallelize and struggle with long sequences \autocite{DBLP:journals/corr/WuSCLNMKCGMKSJL16}.\\
In 2017, \textcite{DBLP:journals/corr/VaswaniSPUJGKP17} have presented the Transformer model which removes the recurrent units and uses only Attention.
This architecture allows for faster training because the encoding step is done in parallel for the entire input.\\
They show that the Transformer can outperform previous RNN-based architectures and achieve state-of-the-art performance for bilingual machine translation \autocite{DBLP:journals/corr/VaswaniSPUJGKP17}.
The study by \textcite{DBLP:journals/corr/abs-1806-06957} suggests that the Transformer performs better than RNNs in the multilingual setting, as well.
Transformer also are successful in other language tasks like semantic analysis or question answering \autocite{DBLP:journals/corr/abs-1810-04805}.
The architecture of the Transformer for translation is an encoder-decoder model, similar to previous RNN-based architectures.
The difference to the recurrent models is, that it replaces these units with multiple stacked blocks of each one Self-Attention and one fully connected layer in the encoder and an additional encoder-decoder Attention layer for each block in the decoder half of the network.\\
Self-Attention is a new layer proposed by \textcite{DBLP:journals/corr/VaswaniSPUJGKP17} which calculates a feature vector for each input part based on its dependency to other parts.
Since the entire input is processed at once, a positional encoding is added at the beginning to enable the network to model the sequential nature of the sentence.

\section{Project Proposal}
For the project, we plan to implement a Transformer model with TensorFlow and Keras based on the model proposed by \textcite{DBLP:journals/corr/VaswaniSPUJGKP17} and extend it for multilingual support as described by \textcite{DBLP:journals/corr/HaNW16,johnson2017google}.
Our code will be publicly available on a GitHub repository\footnote{\url{https://github.com/Aaronsom/deepml-class-project/}}.\\
A possible dataset for our project is the TED corpus, which is a parallel corpus based on 2400 transcribed TED talks available in 109 languages. 
The dataset has been compiled, preprocessed and split in training, dev and test sets by \textcite{Ye2018WordEmbeddings} and is publicly available on their repository\footnote{\url{https://github.com/neulab/word-embeddings-for-nmt}}.
\textcite{DBLP:journals/corr/abs-1903-00089} use this corpus for their study and publish BLEU scores \autocite{papineni2002bleu} for some language pairs which enables us to compare our model against theirs.\\
We are aware of our computational limitations (at minimum one Nvidia GTX960M) compared to a study carried out at Google.
Replication is thus out of scope for us.
Instead, we are interested in how far we can push our model with our limited resources.
We plan to begin experiments on a smaller scale with few languages and extend to more based on the achieved performance.
\\
In more detail, the milestones for our project are:\\
\textbf{(i) English - German Transformer} We implement a working Transformer which we train for English and German until we can receive reasonable translations. 
BLEU scores for English-German are available in other papers to compare our model against.\\
\textbf{(ii) Extend to three to five languages} We extend our model to three to five languages for a more multilingual model. 
Evaluation with BLEU can be a problem for this milestone because not all language pairs have published results. 
We expect this to be the limit of our resources with regards to the number of languages.\\
\textbf{(iiia) Zero Shot Experiments (Optional)} If enough time remains, we experiment with zero shot translations, that is, translations from languages not seen during training. Suitable for this are Germanic or Romance languages similar to our training languages.\\
\textbf{(iiib) Highly Multilingual Model (Optional)} If our hardware and the training time allows for this, then we experiment with highly multilingual models containing up to all 109 languages of the dataset.

\printbibliography[title=References, heading=bibliography]
   
\end{document}
